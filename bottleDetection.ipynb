{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bottleDetection.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"b3urZPo9KZKI"},"source":["# Follow detectron2 setup\n","!pip install pyyaml==5.1\n","\n","import torch\n","TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n","CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n","print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n","\n","# Install detectron2 that matches the above pytorch version\n","# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n","# If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.\n","\n","# exit(0)  # After installation, you may need to \"restart runtime\" in Colab. This line can also restart runtime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K8wAayt3nAth"},"source":["# Setup packages\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import os, json, cv2, random, sys\n","from google.colab.patches import cv2_imshow\n","from google.colab import drive\n","\n","from pycocotools.coco import COCO\n","import requests\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog\n","from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","from detectron2.data import build_detection_test_loader\n","from detectron2.data.datasets import register_coco_instances\n","from detectron2.engine import DefaultTrainer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s6UxUr8CHHFF"},"source":["# Mount drive\n","drive.mount('/content/drive')\n","\n","# Set path to downloaded COCO API files\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks/CS230Project/OpenImages/cocoapi-master/PythonAPI\")\n","sys.path.append(\"/content/drive/My Drive/Colab Notebooks/CS230Project/OpenImages/cocoapi-master/PythonAPI\")\n","\n","# Install downloaded COCO API\n","!python3 setup.py install\n","!pip install awscli"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4NaD5VYHJBUB"},"source":["# Reset path for annotation conversion\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO\")\n","sys.path.append(\"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dXymWUd9fO_Z"},"source":["# Implement convert_annotations.py from openimages2coco github repository, courtesy of the Bethge Lab\n","!python3 convert_annotations.py -p \"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO\" --subsets val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4AFdJ7cmqu_h"},"source":["# Tidy up annotation json to only include desired classes\n","\n","# Filter out so only \"bottle\" remains in train annotation file\n","python3 filter.py --input_json \"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO/anns/instances_train2017.json\" --output_json \"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO/anns/instances_train2017_filtered.json\"  --categories bottle\n","\n","# Filter out so only \"bottle\" remains in \"val\" annotation file (used as development)\n","python3 filter.py --input_json \"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO/anns/instances_val2017.json\" --output_json \"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO/anns/instances_val2017_filtered.json\"  --categories bottle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfsjM7R0fyn0"},"source":["# Selectively download just the bottle images to my drive\n","\n","# instantiate COCO specifying the annotations json path\n","coco = COCO(\"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO/anns/instances_train2017_filtered.json\")\n","# Specify a list of category names of interest\n","catIds = coco.getCatIds(catNms=['bottle'])\n","# Get the corresponding image ids and images using loadImgs\n","imgIds = coco.getImgIds(catIds=catIds)\n","images = coco.loadImgs(imgIds)\n","\n","# Save the images into a local folder (up to 8501 in increments to avoid Google Colab timeout)\n","for im in images[7500:8501]:\n","    img_data = requests.get(im['coco_url']).content\n","    with open(\"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO/ims/train/\" + im['file_name'], 'wb') as handler:\n","        handler.write(img_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDAU9jwkE0vA"},"source":["# Register datasets into detectron2 by calling the annotation file and image path as follows:\n","# register_coco_instances(\"YourTestDatasetName\", {}, \"path to test.json\", \"path to test image folder\")\n","\n","register_coco_instances(\"justBottleCOCOTrain\", {},\"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO/anns/instances_train2017_filtered.json\", \"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO/ims/train\")\n","\n","register_coco_instances(\"justBottleCOCOVal\", {},\"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO/anns/instances_val2017_filtered.json\", \"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO/ims/val\")\n","\n","register_coco_instances(\"COCOVal\", {},\"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO/anns/instances_val2017.json\", \"/content/drive/My Drive/Colab Notebooks/CS230Project/COCO/ims/val\")\n","\n","\n","# Establish dicts and metadata for visualization later\n","bottleData_dicts_Train = DatasetCatalog.get(\"justBottleCOCOTrain\")\n","bottle_metadata_Train = MetadataCatalog.get(\"justBottleCOCOTrain\")\n","\n","bottleData_dicts_Val = DatasetCatalog.get(\"justBottleCOCOVal\")\n","bottle_metadata_Val = MetadataCatalog.get(\"justBottleCOCOVal\")\n","\n","data_dicts_Val = DatasetCatalog.get(\"COCOVal\")\n","metadata_Val = MetadataCatalog.get(\"COCOVal\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkHNxd36JkfT"},"source":["# Ensure visualization and dataset registration was successful by displaying random images and corresponding annotations\n","for d in random.sample(bottleData_dicts_Val, 3):\n","    # print(d)\n","    fn = d[\"file_name\"]\n","    img = cv2.imread(fn)\n","    visualizer = Visualizer(img[:, :, ::-1], metadata=bottle_metadata_Val, scale=0.5)\n","    out = visualizer.draw_dataset_dict(d)\n","    cv2_imshow(out.get_image()[:, :, ::-1])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b3Yh4iHYKLDG"},"source":["# Set up the pretrained configuration for baseline and future training\n","cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n","cfg.DATASETS.TRAIN = (\"justBottleCOCOTrain\",)\n","cfg.DATASETS.TEST = (\"justBottleCOCOVal\",)\n","# cfg.DATALOADER.NUM_WORKERS = 2\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JY9Pxe3ibCxj"},"source":["# Read deployment images for qualitative analysis\n","im1 = cv2.imread('/content/drive/My Drive/Colab Notebooks/CS230Project/rainsCabinetSmall.jpg')\n","im2 = cv2.imread('/content/drive/My Drive/Colab Notebooks/CS230Project/harveyCabinetSmall.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YFa-CyYtCKzb"},"source":["# Establish threshold and pretrained predictor\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n","predictor_init = DefaultPredictor(cfg)\n","\n","# Display predictor's ability to perform inference on \"bottle\" dataset\n","for d in random.sample(bottleData_dicts_Val, 3):    \n","    im = cv2.imread(d[\"file_name\"])\n","    outputs = predictor_init(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n","    v = Visualizer(im[:, :, ::-1],\n","                   metadata=metadata_Val, \n","                   scale=0.5, \n","                  #  instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n","    )\n","    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","    cv2_imshow(out.get_image()[:, :, ::-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ki587-ZfCJzW"},"source":["# Establish training configuration\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (bottle)\n","cfg.TEST.DETECTIONS_PER_IMAGE = 100 # Limit total number of detections to 50\n","\n","cfg.SOLVER.IMS_PER_BATCH = 2 # Set batch size\n","cfg.SOLVER.BASE_LR = 0.001  # Pick a LR\n","cfg.SOLVER.MOMENTUM = 0.95 # Pick beta for momentum\n","cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy problem\n","cfg.SOLVER.STEPS = [] #(200,)        # either do or dont decay learning rate at step 200\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256   # faster, and good enough for this toy problem (default: 512)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oar-AY5jWu80"},"source":["# Train\n","\n","cfg.SOLVER.CHECKPOINT_PERIOD = 50 # Make sure we can resume if training fails for any reason\n","\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","resume_dir = os.getcwd()+'/output/'\n","trainer = DefaultTrainer(cfg) \n","trainer.resume_or_load(resume=False) #Try \"resume_dir\" here if wanting to resume; \"resume=False\" if starting from scratch\n","trainer.train()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5VWcQbuZK8JY"},"source":["# Look at training curves in tensorboard:\n","%load_ext tensorboard\n","%tensorboard --logdir output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VOOr5AaFNf0q"},"source":["# Inference should use the config with parameters that are used in training\n","# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n","cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n","predictor = DefaultPredictor(cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5bYaHPiN-h6"},"source":["# Visualize new performance on random examples\n","for d in random.sample(bottleData_dicts_Val, 3):    \n","    im = cv2.imread(d[\"file_name\"])\n","    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n","    v = Visualizer(im[:, :, ::-1],\n","                   metadata=bottle_metadata_Val, \n","                   scale=0.5, \n","                  #  instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n","    )\n","    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","    cv2_imshow(out.get_image()[:, :, ::-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dep2h7XBNXJ3"},"source":["cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # reset threshold if needed\n"," \n","# Using the pretrained model, perform inference on deployment images and visualize\n","\n","# Example 1\n","output1 = predictor_init(im1)\n","\n","# Use `Visualizer` to draw the predictions on the image.\n","v = Visualizer(im1[:, :, ::-1], metadata_Val, scale=1.2) # MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n","out = v.draw_instance_predictions(output1[\"instances\"].to(\"cpu\"))\n","\n","# Display and save the annotated image\n","out_im = out.get_image()[:, :, ::-1]\n","cv2_imshow(out_im)\n","\n","# Example 2\n","output2 = predictor_init(im2)\n","\n","# Use `Visualizer` to draw the predictions on the image.\n","v = Visualizer(im2[:, :, ::-1], metadata_Val, scale=1.2) # MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n","out = v.draw_instance_predictions(output2[\"instances\"].to(\"cpu\"))\n","\n","# Display and save the annotated image\n","out_im = out.get_image()[:, :, ::-1]\n","cv2_imshow(out_im)\n","\n","\n","\n","# Using the model after transfer learning, perform inference on deployment images and visualize\n","\n","# Example 1\n","output1 = predictor(im1)\n","\n","# Use `Visualizer` to draw the predictions on the image.\n","v = Visualizer(im1[:, :, ::-1], bottle_metadata_Val, scale=1.2) # MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n","out = v.draw_instance_predictions(output1[\"instances\"].to(\"cpu\"))\n","\n","# Display and save the annotated image\n","out_im = out.get_image()[:, :, ::-1]\n","cv2_imshow(out_im)\n","\n","# Example 2\n","output2 = predictor(im2)\n","\n","# Use `Visualizer` to draw the predictions on the image.\n","v = Visualizer(im2[:, :, ::-1], bottle_metadata_Val, scale=1.2) # MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n","out = v.draw_instance_predictions(output2[\"instances\"].to(\"cpu\"))\n","\n","# Display and save the annotated image\n","out_im = out.get_image()[:, :, ::-1]\n","cv2_imshow(out_im)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJ8rgRevOfcx"},"source":["# Evaluate the configuration to obtain an AP\n","\n","evaluator = COCOEvaluator(\"justBottleCOCOVal\", output_dir=\"./output\")\n","val_loader = build_detection_test_loader(cfg, \"justBottleCOCOVal\")\n","print(inference_on_dataset(predictor.model, val_loader, evaluator))\n","# another equivalent way to evaluate the model is to use `trainer.test`"],"execution_count":null,"outputs":[]}]}